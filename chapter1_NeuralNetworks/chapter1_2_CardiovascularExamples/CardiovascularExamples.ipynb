{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerual networks applied to cardiovascular models\n",
    "\n",
    "Neural networks can as we saw in chapter 1.1 be used as function approximators, and may similarily learn how to approximate the behaviour of mathematical models. In this section we will provide examples of neural networks applied to some cardiovascular models for the systemic circulation, and for blood vessel stenosis models. In addition to tensorflow we also apply the Keras library here, which is a higher level package simlipfying many of the tasks that can be performed with tensorflow. \n",
    "\n",
    "---\n",
    "\n",
    "## The Two Element Windkessel Model\n",
    "\n",
    "Here we will implement the two element Windkessel in a Neural Network. The model is characterized by the parameters resistance $R$ and compliance $C$. These parameters are often interpreted as total vascular resistance and systemic arterial compliance, when the model represents the systemic circulation. The equation describing the model is as follows;\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\mathrm{d}p}{\\mathrm{dt}} = \\frac{Q_{\\mathrm{in}}}{C} - \\frac{p}{R C}, \n",
    "\\end{equation*}\n",
    "\n",
    "where $Q_{\\mathrm{in}}$ is the flow entering the compliant volume at a given point in time. The compliant vessels of the systemic circulation are filled and stressed by blood ejected in systole from the heart. During diastole the elastic recoil of the vessel walls smoothes the blood flow in the vessels and keeps the circulation flowing even when the heart is not ejecting blood. This effect is named after an air chamber used to produce smooth water flows for fire fighting in the past. See the Figure below for a simple illustration, comparing the blood vessels to the original Windkessel. \n",
    "\n",
    "<img src = \"fig/Windkessel_effect.svg\" width=600>\n",
    "\n",
    "[Image source](https://en.wikipedia.org/wiki/Windkessel_effect)\n",
    "\n",
    "The model computes the blood pressure waveform of the systemic arteries $P_{ao}(t)$ (used interchangeably with $p$) according to the imposed inflow $Q_{\\mathrm{in}}$, and the specified parameters $R$ and $C$. If we use this model to produce data then we can train a NN to appoximate the model. \n",
    "\n",
    "which ideally should be equal to zero, and must be minimized to obey the physics of the problem. $u$ refers to our variable of interest, which here is $p$. \n",
    "\n",
    "Problem 1:\n",
    "- Tune the hyperparameters; number of nodes, layers, epochs, training rate, but also the number of training points and try to get an optimal recreation of the model. You can also try feeding the model a new data set and see if the model can reproduce itself. \n",
    "\n",
    "For this problem a gaussian inflow is prescribed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import joblib\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "from plotScripts import plotScatterGeneric, plotBAGeneric\n",
    "\n",
    "from sklearn import preprocessing\n",
    "tf.logging.set_verbosity(tf.logging.ERROR) # depreciate warnings\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load data and split training set into actual training and validation sets\n",
    "#===================================================\n",
    "dataNameTrain = \"WK2DataLabeled_train\" # for training the model\n",
    "dataNameTest = \"WK2DataLabeled_test\" # for testing the model\n",
    "\n",
    "testFracTest = 0.3\n",
    "colNames = [\"C\", \"R\", \"P_sys\", \"P_dia\", \"PP\", \"Hyp\"]\n",
    "featureCols = [0, 1] # C and R\n",
    "labelCol = 4 # PP\n",
    "\n",
    "data = np.genfromtxt(dataNameTrain, skip_header=True)\n",
    "x, y = data[:,featureCols].copy(), data[:,labelCol].copy()\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(x, y, test_size=testFracTest, random_state=42)\n",
    "#===================================================\n",
    "# scale inputs\n",
    "#===================================================\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X) # fit (find mu and std) scaler and transform data\n",
    "test_X  = scaler.transform(test_X) # transform data based on mu and std from training/learning set\n",
    "#===================================================\n",
    "# set parameters for Neural net\n",
    "#===================================================\n",
    "afFunc = \"relu\"\n",
    "opt = 'adam'\n",
    "lossFunc= 'mean_squared_error'\n",
    "max_epochs = 1000\n",
    "batch_size = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set up Neural net\n",
    "#===================================================\n",
    "model = keras.Sequential()\n",
    " \n",
    "model.add(keras.layers.Dense(50, activation=afFunc, input_dim=2))\n",
    "model.add(keras.layers.Dense(50, activation=afFunc))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(optimizer=opt,\n",
    "              loss=lossFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train model using mini-batches\n",
    "#===================================================\n",
    "trainLoss = []\n",
    "testLoss = []\n",
    "for n in range(max_epochs):\n",
    "    for batch in iterate_minibatches(train_X, train_Y, batch_size, shuffle=True):\n",
    "        x_batch, y_batch = batch \n",
    "        \n",
    "        tmpLoss = model.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "    trainLoss.append(model.evaluate(train_X, train_Y))\n",
    "    testLoss.append(model.evaluate(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot loss and predictions\n",
    "#===================================================\n",
    "plt.figure()\n",
    "plt.plot(trainLoss, \"o\")\n",
    "plt.plot(testLoss, \"o\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend([\"train\", \"test\"])\n",
    "\n",
    "test_Y_pred = model.predict(test_X).flatten()\n",
    "plotScatterGeneric(test_Y, test_Y_pred, \"PP\", \"PP_ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coronary artery stenoses\n",
    "\n",
    "In this example we'll be working with pressure drops along coronary artery stenoses. A stenosis is a narrowing or partial obstruction in the arteries that feed the heart muscle, as illustrated in the figure below, where normal segments are marked as red, and stenotic segments in grey and red depending on the severeness. Such stenoses impedes flow to the heart muscles, and may eventually cause myocardial infarction. In this example we'll look at the pressure drop across such stenoses, where predictions are based on an algebraic stenoses model given by:\n",
    "\n",
    "\\begin{equation}\n",
    " \\Delta P_\\mathrm{0D} = a_\\mathrm{0D} Q + b_\\mathrm{0D} Q^2\\,,\n",
    " \\label{eq:dplinearquadratic}\n",
    "\\end{equation}\n",
    "with parameters \n",
    "\\begin{equation}\n",
    " a_\\mathrm{0D} = \\frac{K_v \\, \\mu}{A_0 \\,D_0}\\,,\\quad \\quad b_\\mathrm{0D}=\\frac{K_t \\rho}{2 \\, A_0^2}\\left(\\frac{A_0}{A_{s}} -1\\right)^2\\,,\n",
    " \\label{eq:aAndb}\n",
    "\\end{equation}\n",
    " where $A_0$ and $A_{s}$ refer to cross-sectional areas of the normal (average of inlet and outlet) and stenotic segments, respectively. Similarly, $D_0$ and $D_{s}$ represent the normal and stenotic diameters, respectively. Furthermore, $K_v$ and $K_t$ are empirical coefficients, with $K_v = 32\\left(0.83\\,L_{s} + 1.64 \\,D_{s}\\right)\\cdot\\left(A_0/A_{s}\\right)^2/D_0$, $K_t=1.52$, whereas $L_{s}$ is the length of the length of the stenotic segment. [Seeley and Young 1973](https://www.sciencedirect.com/science/article/pii/0021929076900865). Thus the pressure drop is a function of inlet, outlet and minimum radius/cross-section, lenght and flow-rate. In this dataset we have computed approximately 5000 different pressure losses, $\\Delta P_\\mathrm{0D}$ for realistic inputs, and the goal is to train a neural network to learn the function, $\\Delta P_\\mathrm{0D}$.\n",
    "\n",
    "<img src=\"fig/coronaryStenoses.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem\n",
    "- Train a neural network with 2 hidden networks with 100 neurons each with relu activation function\n",
    "- Compare the predictions on the training and testing data. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load training and testing data\n",
    "#===================================================\n",
    "dataNameTrain = \"stenosesData_train_mod\" # for training the model\n",
    "dataNameTest = \"stenosesData_test_mod\" # for testing the model\n",
    "\n",
    "colNames = [\"r0\", \"rMin\", \"l\", \"Q\", \"DP0D\"]\n",
    "\n",
    "featureCols = [0, 1, 2, 3] #\n",
    "labelCol =  4\n",
    "\n",
    "data = np.genfromtxt(dataNameTrain, skip_header=True)\n",
    "train_X, train_Y = data[:,featureCols].copy(), data[:,labelCol].copy()\n",
    "data_test = np.genfromtxt(dataNameTest, skip_header=True)\n",
    "test_X, test_Y = data_test[:,featureCols].copy(), data_test[:,labelCol].copy()\n",
    "#===================================================\n",
    "# scale inputs\n",
    "#===================================================\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X) # fit (find mu and std) scaler and transform data\n",
    "test_X  = scaler.transform(test_X) # transform data based on mu and std from training/learning set\n",
    "#===================================================\n",
    "# set parameters for Neural net\n",
    "#===================================================\n",
    "afFunc = \"relu\"\n",
    "opt = 'adam'\n",
    "lossFunc= 'mse'\n",
    "max_epochs = 1000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set up Neural net\n",
    "#===================================================\n",
    "model = keras.Sequential()\n",
    " \n",
    "model.add(keras.layers.Dense(100, activation=afFunc, input_dim=len(featureCols)))\n",
    "model.add(keras.layers.Dense(100, activation=afFunc))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(optimizer=opt,\n",
    "              loss=lossFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train model using mini-batches\n",
    "#===================================================\n",
    "trainLoss = []\n",
    "testLoss = []\n",
    "for n in range(max_epochs):\n",
    "    for batch in iterate_minibatches(train_X, train_Y, batch_size, shuffle=True):\n",
    "        x_batch, y_batch = batch \n",
    "        \n",
    "        tmpLoss = model.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "    trainLoss.append(model.evaluate(train_X, train_Y))\n",
    "    testLoss.append(model.evaluate(test_X, test_Y))\n",
    "    \n",
    "#===================================================\n",
    "# Note that keras can perform the for loop and splitting into mini-bathches \n",
    "# in a \"one-liner\" like the one below. We'll use this in the examples below\n",
    "#===================================================\n",
    "#model.fit(train_X, train_Y, epochs=max_epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that keras can do run the for loop and splitting into mini-bathches in a \"one-liner\" like the one below. We'll use this in the examples below:\n",
    "```model.fit(train_X, train_Y, epochs=max_epochs, batch_size=batch_size, verbose=1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot loss and predictions\n",
    "#===================================================\n",
    "plt.figure()\n",
    "plt.plot(trainLoss, \"o\")\n",
    "plt.plot(testLoss, \"o\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim([0, min(testLoss) + 1*np.std(testLoss)])\n",
    "plt.legend([\"train\", \"test\"])\n",
    "\n",
    "test_Y_pred = model.predict(test_X).flatten()\n",
    "train_Y_pred = model.predict(train_X).flatten()\n",
    "plotScatterGeneric(train_Y, train_Y_pred, \"DP\", \"DP_ML\")#, title=\"predictions on training set\")\n",
    "plotScatterGeneric(test_Y, test_Y_pred, \"DP\", \"DP_ML\")#, title=\"predictions on testing set\")\n",
    "plotBAGeneric(test_Y, test_Y_pred, \"DP\", \"DP_ML\")#, title=\"predictions on testing set\")\n",
    "\n",
    "print(\"train loss: {}, test loss: {}\".format(model.evaluate(train_X, train_Y),\n",
    "                                             model.evaluate(test_X, test_Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving generalization\n",
    "Neaural networks are great at learning to fit data as exemplified above; with enough layers and neurons we are able to fit the training data very well. However, when predictions are made on samples that were not used in training the networks perform much worse. This is known as __overfitting__ and is a __very important__ concept withtin machine learning and neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section will introduce some concepts that can be used to prevent overfitting\n",
    "- Reduce the complexity of the network. In general more complex networks are more prone to overfitting\n",
    "- Monitor the predictions (loss) on data that is not used in the training, by introducing a validation set \n",
    "- Introduce regularization\n",
    "- Introduce dropout\n",
    "\n",
    "### Methods to prevent overfitting\n",
    "\n",
    "#### Introduce a validation set \n",
    "- A validation set is a certain fraction of the training set that is not used to estimate gradients and update weights and biases, but is used to test the networks after each epoch. If the loss on the actual training data continous to go towards zero but the validation loss increases, this is a sign of overfitting.\n",
    "- The introduction of a validation sthet requires that some of the training data is set aside, and thus not used for training. It is however a good rule to use split the training data into trainig and validation set\n",
    "- One still need to test the network on unseen data, and we therefor also use a test set; i.e. we have a training data that is used to update the weights, we have validation data that is used to see when overfitting occurs, and we have testing data, to test on \"completely\" unseen data\n",
    "\n",
    "#### Regularization\n",
    "Overfitting is associated with large weights, and may thus be reduced by constraining large weights. Remember that neural network as trained by minimizing a loss function, i.e. the mean squared errors of predictions:\n",
    "\\begin{equation} \\label{eq:lossFunc}\n",
    "E(\\omega_1, b_1 \\ldots \\omega_L, b_L) = \\frac{1}{N}\\sum_j^N ( y_j - \\hat{y_j} )^2\n",
    "\\end{equation}\n",
    "\n",
    "Now with regularization we penalize large weights by adding a second term, e.g. the $L_2$ norm of the weights:\n",
    "\\begin{equation}\\label{eq:lossFunc_reg}\n",
    "E(\\omega_1, b_1 \\ldots \\omega_L, b_L) = \\frac{1}{N}\\sum_j^N ( y_j - \\hat{y_j} )^2 + \\frac{\\gamma}{N}\\sum_l^L  \\omega_l ^2 \n",
    "\\end{equation}\n",
    "The idea is that by penalizing big weights you prevent the network to try to fit too complex functions (i.e. try to fit noise etc.). [Check out this post to read more about regularization](https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3)\n",
    "\n",
    "#### Dropout\n",
    "Dropout is another regularization technique proposed by [Srivastava et al. 2014](http://jmlr.org/papers/v15/srivastava14a.html). Dropout involves randomely removing a certain fraction of the activation of a neuron in a layer. [The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### problem\n",
    "- In the code below we have split the training-set into an actual training and validation set. Use the same setting as the above case 2 hidden layers with 100 neurons each with sigmoid activation function. \n",
    "- Reduce the complexity of the network. e.g. use 2 hidden layers with 25 inputs. \n",
    "- add a regularization term (explore with differend values of $\\gamma$) to the hidden layers\n",
    "- Add dropout to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load training, validation and testing data\n",
    "#===================================================\n",
    "\n",
    "val_frac = 0.2\n",
    "\n",
    "data = np.genfromtxt(dataNameTrain, skip_header=1)\n",
    "data_test = np.genfromtxt(dataNameTest, skip_header=1)\n",
    "\n",
    "\n",
    "x, y = data[:,featureCols].copy(), data[:,labelCol].copy()\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(x, y, test_size=val_frac, random_state=42)\n",
    "\n",
    "test_X, test_Y = data_test[:,featureCols].copy(), data_test[:,labelCol].copy()\n",
    "\n",
    "\n",
    "#===================================================\n",
    "# scale inputs\n",
    "#===================================================\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train_X = scaler.fit_transform(train_X) # fit (find mu and std) scaler and transform data\n",
    "test_X  = scaler.transform(test_X) # transform data based on mu and std from training/learning set\n",
    "val_X  = scaler.transform(val_X) # transform data based on mu and std from training/learning set\n",
    "\n",
    "train_Y.resize(train_Y.size, 1)\n",
    "test_Y.resize(test_Y.size, 1)\n",
    "val_Y.resize(val_Y.size, 1)\n",
    "\n",
    "scaler_y = preprocessing.StandardScaler()\n",
    "train_Y = scaler_y.fit_transform(train_Y) # fit (find mu and std) scaler and transform data\n",
    "test_Y = scaler_y.transform(test_Y) # transform data based on mu and std from training/learning set\n",
    "val_Y = scaler_y.transform(val_Y) # transform data based on mu and std from training/learning set\n",
    "\n",
    "#===================================================\n",
    "# set parameters for Neural net\n",
    "#===================================================\n",
    "afFunc = \"relu\"\n",
    "opt = 'adam'\n",
    "lossFunc= 'mse'\n",
    "max_epochs = 1000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set up Neural net with regularization and dropout\n",
    "#===================================================\n",
    "dropOutActive = True\n",
    "dropoutVal = 0.1\n",
    "gamma = 0.000001 # if reguVal is set to zero no regularization is used\n",
    "model = keras.Sequential()\n",
    "N_neurons = 25\n",
    "\n",
    "model.add(keras.layers.Dense(N_neurons, activation=afFunc, input_dim=len(featureCols), \n",
    "                             kernel_regularizer=regularizers.l2(gamma)))\n",
    "if dropOutActive:\n",
    "    model.add(keras.layers.Dropout(dropoutVal))\n",
    "model.add(keras.layers.Dense(N_neurons, activation=afFunc, kernel_regularizer=regularizers.l2(gamma)))\n",
    "if dropOutActive:\n",
    "    model.add(keras.layers.Dropout(dropoutVal))\n",
    "model.add(keras.layers.Dense(1, activation='linear'))\n",
    "model.compile(optimizer=opt,\n",
    "              loss=lossFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set output and stop criteria\n",
    "#===================================================\n",
    "\n",
    "# save training loss and validation loss to a file\n",
    "logger = keras.callbacks.CSVLogger('costVsEpochs', append=False, separator=' ')\n",
    "# save best weights to file (as monitored by the validation loss)\n",
    "filepath = 'bestWeights.hdf5'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                                             mode='min')\n",
    "callbacks = [logger, checkpoint]\n",
    "#===================================================\n",
    "# train model by opdating weights based on gradients in the actualt training data \n",
    "# and monitoring the error in the validation set. Note that the test set is not used at all\n",
    "# we'll save that to see how the network perform on data that was not used in updating weights or in the \n",
    "# minimization of the loss\n",
    "#===================================================\n",
    "model.fit(train_X, train_Y, epochs=max_epochs, batch_size=batch_size,\n",
    "          verbose=1, validation_data=(val_X, val_Y), shuffle=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot loss and predictions\n",
    "#===================================================\n",
    "model.load_weights('bestWeights.hdf5')\n",
    "costVsEpochs = np.genfromtxt(\"costVsEpochs\", skip_header=1)\n",
    "trainLoss = costVsEpochs[:, 1]\n",
    "valLoss = costVsEpochs[:, 2]\n",
    "plt.figure()\n",
    "plt.plot(trainLoss, \"o\")\n",
    "plt.plot(valLoss, \"o\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim([0, min(valLoss) + 1*np.std(valLoss)])\n",
    "plt.legend([\"train\", \"test\"])\n",
    "\n",
    "test_Y_pred = model.predict(test_X).flatten()\n",
    "train_Y_pred = model.predict(train_X).flatten()\n",
    "val_Y_pred = model.predict(val_X).flatten()\n",
    "\n",
    "train_Y_pred = scaler_y.inverse_transform(train_Y_pred).flatten()\n",
    "test_Y_pred = scaler_y.inverse_transform(test_Y_pred).flatten()\n",
    "val_Y_pred = scaler_y.inverse_transform(val_Y_pred).flatten()\n",
    "\n",
    "train_Y_plot = scaler_y.inverse_transform(train_Y).flatten()\n",
    "test_Y_plot = scaler_y.inverse_transform(test_Y).flatten()\n",
    "\n",
    "val_Y_plot = scaler_y.inverse_transform(val_Y).flatten()\n",
    "\n",
    "#train_Y_plot = train_Y.flatten() \n",
    "#test_Y_plot = test_Y.flatten() \n",
    "\n",
    "#val_Y_plot = val_Y.flatten() \n",
    "\n",
    "plt.figure()\n",
    "plotScatterGeneric(train_Y_plot, train_Y_pred, \"DP\", \"DP_ML\")#, title=\"predictions on training set\")\n",
    "plotScatterGeneric(test_Y_plot, test_Y_pred, \"DP\", \"DP_ML\")#, title=\"predictions on testing set\")\n",
    "plotScatterGeneric(val_Y_plot, val_Y_pred, \"DP\", \"DP_ML\")#, title=\"predictions on validation set\")\n",
    "plotBAGeneric(test_Y_plot, test_Y_pred, \"DP\", \"DP_ML\")\n",
    "\n",
    "print(\"train loss: {}, val loss: {}, test loss: {}\".format(model.evaluate(train_X, train_Y), \n",
    "                                                           model.evaluate(val_X, val_Y),\n",
    "                                                           model.evaluate(test_X, test_Y)))\n",
    "print(\"train DP: {}, val DP: {}, test DP: {}\".format(np.average(train_Y_plot), \n",
    "                                                     np.average(val_Y_plot),\n",
    "                                                     np.average(test_Y_plot)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
