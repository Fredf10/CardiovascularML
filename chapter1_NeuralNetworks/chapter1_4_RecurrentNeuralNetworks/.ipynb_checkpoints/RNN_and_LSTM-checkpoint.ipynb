{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN)\n",
    "## - and Long Short Term Memory (LSTM)\n",
    "\n",
    "In this notebook we'll have a look at time-series and reccurent neural networks (RNN) and in particular LSTM, which is one subcatergory of RNNs.\n",
    "\n",
    "The strength of recurrent neural networks is that it can keep information that it learned from a previous element in a list to inform the current element you are training on or evaluating. Therefore it is best used in problems where you will be analyzing or predicting the future values of a list or a data sequence. In contrast an ordinary NN will remember what it has trained, but does not remember anything about the sequence in which it evaluates inputs. \n",
    "\n",
    "LSTM networks has a more sophisticated memory mechanisms enabling the network to remember information that is further apart in time, or distance within the sequence, but to better illustrate what is new with RNNs and LSTM considerthe figures below. \n",
    "\n",
    "Figure 1 shows the architecture of normal or \"vanilla\" NNs in sequence that takes the sequence of inputs, but has no interactions between each input evaluation. Many networks could be trained for each sequence element separately, but not be able to remember anything about the previous sequence elements. \n",
    "\n",
    "<img src=\"Fig1Vanilla.png\" width=700>\n",
    "<center>Figure 1</center>\n",
    "    \n",
    "[Image source](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)\n",
    "\n",
    "Figure 2 introduces the notion of RNNs, the same weights $W_x$ are trained and applied to all inputs of the sequence. The set of weights $W_h$ are trained to take the relevant information from one input evaluation to the next in the series. \n",
    "\n",
    "<img src=\"Fig2RNN.png\" width=700>\n",
    "<center>Figure 2</center>\n",
    "\n",
    "[Image source](https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce)\n",
    "\n",
    "What LSTM introduces to the network is to add a cell state that can carry information from all previous input evaluations and has a more sophisticated way of setting the wieghts $W_h$, we refer to the referenced links and sources to get more detail on this. \n",
    "\n",
    "[Source](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "Useful links:\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce\n",
    "- https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "We'll first test how a simple LSTM network can learn to predict y[n + 1] based on a synthetical a cosine wave with exponential amplitude. We'll then have a look at a time-series from invasive pressure measurements.\n",
    "\n",
    "Instuctions:\n",
    "- The code is configured to run the exponential amplitude cosine example. Run all cells to observe the performance. The RNN LSTM trains on the train fraction of the data, which at default is set to half the data, and tests on the remaining fraction. \n",
    "- Examine the code to understand how the network uses data to train, and tune the training fraction and other paramaters and see how this affects the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "t_start = 0\n",
    "t_end = 2*np.pi\n",
    "N = 1001\n",
    "t = np.linspace(t_start, t_end, N)\n",
    "y = np.cos(5*t)*np.exp(2*t/t_end)\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(8,4),dpi=300)\n",
    "plt.plot(t, y)\n",
    "plt.title('Data series')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "y = y[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    # dataX contains y_value of current time-step in addition to the previous (look_back - 1) time-steps\n",
    "    # dataY contains the y_value of the next time-step\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Parameters\n",
    "look_back = 10\n",
    "epochs = 10\n",
    "trainFrac = 0.5\n",
    "LSTM_blocks = 4\n",
    "\n",
    "# Scale data to be bounded by 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y = scaler.fit_transform(y)\n",
    "# Split into train and test sets\n",
    "train_size = int(len(y) * trainFrac)\n",
    "test_size = len(y) - train_size\n",
    "y_train, y_test = y[0:train_size,:], y[train_size:len(y),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = create_dataset(y_train, look_back)\n",
    "testX, testY = create_dataset(y_test, look_back)\n",
    "# turn 2D (N_train, look_back) array into 3D array of shape (N_train, 1, look_back)\n",
    "# dims should be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1])) \n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "trainY = trainY[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "testY = testY[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the neural network architecture \n",
    "model = Sequential()\n",
    "model.add(LSTM(LSTM_blocks, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY)\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[:,0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[:,0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(y)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(y)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(y)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.figure(1, figsize=(8,4),dpi=300)\n",
    "plt.plot(scaler.inverse_transform(y), label=\"Data\")\n",
    "plt.plot(trainPredictPlot, label=\"Training data prediction\")\n",
    "plt.plot(testPredictPlot, label=\"Test data prediction\")\n",
    "plt.title('Data series')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code was inspired by this [source](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
