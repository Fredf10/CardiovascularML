{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to physics informed neural networks (PINNS)\n",
    "In the previous examples we have trained neural networks to represent functions by feeding it with a set of observations (samples) at certain input points $x, y(x)$, and we have minimized the mean squared error (MSE) between observations and predictions.\n",
    "This may require many training samples, particularly in the case of complex functions. However, if we have prior information regarding the physical behaviour of $y$ such information could be used in combination with observations when training a neural network. In this setting recent developments in [automatic differentiation](https://arxiv.org/abs/1502.05767) can be exploited, which allow to differentiate neural networks with respect to their input coordinates to obtain [physics informed neural networks](https://arxiv.org/abs/1711.10561) (PINNS). In this setting let us consider a general nonlinear partial differential equation of the general form\n",
    "\n",
    "\\begin{equation}\n",
    "u_t +  \\Lambda \\left[u;\\lambda \\right] = 0\\,,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Lambda$ is a differential operator, and $\\lambda$ are parameters. Like for regular neural networks we would like to minimize discenpencies between observations $u\\left(x_i, t^n \\right)$ and predictions, however we would also like that our governing equation is satisfied, and hence want to minimize the left hand side of the govering equation:\n",
    "\\begin{equation}\n",
    "f := u_t +  \\Lambda \\left[ u \\right] \\,,\n",
    "\\end{equation}\n",
    "\n",
    "In order to obtain a loss function\n",
    "\\begin{equation}\n",
    "loss  = \\mathrm{MSE}_u + \\mathrm{MSE}_f = \\frac{1}{N_u}\\sum_{i=1}^{N_u}\\left(u\\left(t_u^i, x_u^i\\right) - u_i\\right)^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\left(f\\left(t_f^i, x_f^i\\right)\\right)^2  \\,,\n",
    "\\end{equation}\n",
    "\n",
    "i.e. by evaluting the loss function based on the neural nets performance for predicting $u$, a set of $\\left\\{t_u^i, x_u^i, u^i\\right\\}_{i=1}^{N_u}$ training points (initial and boundary data) and by additionally evaluating the left hand side of the governing equation, $f$ on a set of points $\\left\\{t_f^i, x_f^i\\right\\}_{i=1}^{N_f}$.\n",
    "\n",
    "## Solve ODEs by minimizing a combination of observations and physical (equations) \n",
    "\n",
    "In order to introuduce PINNS we'll start by training a neural net to represent the exponential function $y\\left(x\\right) = e^x$. We'll start by learning to represent the function from a set of (training) points, y. We'll then see how we can train the same network by constraining it to satisfy (or at least minimize deviation from) it's governing equation \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d \\,y\\left(x\\right)}{dx}  - y\\left(x\\right) = 0  \\,.\n",
    "\\end{equation}\n",
    "\n",
    " In this case we have only one independent variable, $x$, and the loss function for the regual neural network, and for the PINN simplify to:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{loss}_\\mathrm{NN} =  \\mathrm{MSE}_y  = \\frac{1}{N_y}\\sum_{i=1}^{N_y}\\left(u\\left(x_y^i\\right) - y_i\\right)^2  \\,,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{loss}_\\mathrm{PINN}  = \\mathrm{MSE}_u + \\mathrm{MSE}_f = \\frac{1}{N_{y, PINN}}\\sum_{i=1}^{N_{y, PINN}}\\left(u\\left(x_y^i\\right) - y_i\\right)^2 + \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\left(f\\left(x_f^i\\right)\\right)^2  \\,,\n",
    "\\end{equation}\n",
    "\n",
    "where $f = y_x - y$. Note that the number of observations $N_y$ and $N_{y, PINN}$ does not have to be the same in $\\mathrm{loss}_\\mathrm{NN}$ and $\\mathrm{loss}_\\mathrm{PINN}$, since in general we would like to use PINNS in cases where we have sparse amount of data/observations. In fact (in this example) we will only provide the PINN with one observed $y$ value. The number of points $N_f$ where $f$ is evaluated, on the other hand, is not constrained by lack of data, and we are in principle free to chose as many points as we want. But in this example we'll compare the regular NN  and PINN in cases where $N_{y, PINN} + N_f \\approx N_y$.\n",
    "\n",
    "In both cases we'll use a single hidden layer, like the one below, however the number of neurons can be changed.\n",
    "\n",
    "\n",
    "<img src=\"fig/neuralNet.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "- Compare accuracy (MSE) using the regular NN and PINN\n",
    "- Compare accuracy (MSE) using the regular NN and PINN in the case of extrapolation, i.e. when training is done on a limited set of data, and predictions are obtained outside this area.\n",
    "- Modify the code to represent $y(x)$ = cos(x), in which $f = y + \\frac{d^2\\,y}{dx^2}$\n",
    "- Modify the code to represent y(x) = e^x * cos(x), in which $f: y - \\frac{dy}{dx} + 0.5\\,\\frac{d^2 \\, y}{dx^2}$\n",
    "- In the latter two cases, try to increase the domain bounds. Also try with and without feeding the NN with scaled (normalized between 0 and 1) inputs. For the PINN you may need to feed it with a few more observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#==========================================================================================#\n",
    "# Set parampeters (hyperameters) and compute a training data set for the desired function  #\n",
    "#==========================================================================================#\n",
    "\n",
    "N = 10001 # Number of training points\n",
    "N_train = 10 # Number of points to train on per training attempt\n",
    "N_neurons = 5 # Number of neurons in the hidden layer\n",
    "afunc = tf.nn.sigmoid # Set activation function, other functions .relu, .sigmoid, .tanh, .elu\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 5000 # Number of epochs\n",
    "\n",
    "#Set x-domain boundaries\n",
    "x_start = 0.\n",
    "x_end = 2.\n",
    "\n",
    "#ub = np.array([[x_end]]) #upper bound\n",
    "x_data = np.linspace(x_start, x_end, N)\n",
    "#===================================================\n",
    "# Specify what function you wish to train for\n",
    "#===================================================\n",
    "y_data = np.exp(x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "y_data = y_data[:, np.newaxis] # turn 1D array into 2D array of shape (N, 1)\n",
    "\n",
    "constrainUpper = 1. # set smaller than one to test extrapolation\n",
    "idx = np.random.choice(int(x_data.shape[0]*constrainUpper), N_train, replace=False)\n",
    "x_data_train = x_data[idx]\n",
    "y_data_train = y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set up placeholder for inputs and outputs\n",
    "#===================================================\n",
    "x = tf.placeholder(tf.float32, shape=[None, x_data.shape[1]], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, y_data.shape[1]], name='y')\n",
    "#===================================================\n",
    "# declare weights and biases input --> hidden layer\n",
    "#===================================================\n",
    "W1 = tf.Variable(tf.random_normal([1, N_neurons], stddev=0.5), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([N_neurons]), name='b1')\n",
    "#===================================================\n",
    "# declare weights and biases of hidden --> output layer\n",
    "#===================================================\n",
    "W2 = tf.Variable(tf.random_normal([N_neurons, 1], stddev=0.5), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='b2')\n",
    "#===================================================\n",
    "# declare output of NN\n",
    "#===================================================\n",
    "a1 = afunc(tf.add(tf.matmul(x, W1), b1)) # activation of hidden layer\n",
    "y_NN = tf.add(tf.matmul(a1, W2), b2) # computational graph for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    y_pred_init = sess.run(y_NN, feed_dict = {x:x_data})\n",
    "plt.figure()\n",
    "plt.plot(x_data.flatten(), y_data.flatten(), label=\"Solution\")\n",
    "plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'o', label = 'Training points')\n",
    "plt.plot(x_data.flatten(), y_pred_init.flatten(), label='Initialization values')\n",
    "plt.title('Initial state and training points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using regular NN\n",
    "#===================================================\n",
    "loss = tf.reduce_mean(tf.square(y - y_NN))\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        _, c = sess.run([optimiser, loss], \n",
    "                     feed_dict={x: x_data_train, y: y_data_train})\n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost))\n",
    "\n",
    "    y_pred = sess.run(y_NN, feed_dict = {x:x_data, y:y_data})\n",
    "    loss_pred = sess.run(loss, feed_dict = {x:x_data, y:y_data})\n",
    "    plt.figure()\n",
    "    plt.plot(x_data.flatten(), y_data.flatten(), 'o')\n",
    "    plt.plot(x_data.flatten(), y_pred.flatten(),'r--')\n",
    "    plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'bo')\n",
    "    plt.title('Regular Neural Network Solution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend([\"y\", \"y_NN\", \"y_train\"])\n",
    "    plt.show()\n",
    "    print(\"MSE_u on all data (N: {}): \".format(x_data.shape[0]), loss_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    y_pred_init = sess.run(y_NN, feed_dict = {x:x_data})\n",
    "plt.figure()\n",
    "plt.plot(x_data.flatten(), y_data.flatten(), label='Solution')\n",
    "plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'o', label='Training points')\n",
    "plt.plot(x_data.flatten(), y_pred_init.flatten(), label='Initialization values')\n",
    "plt.title('Initial state and training points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using PINNS\n",
    "#===================================================\n",
    "\n",
    "# Define placeholders for the neural net estimating the differential equation\n",
    "x_f = tf.placeholder(tf.float32, shape=[None, x_data.shape[1]], name='x_f')\n",
    "a1_PINNS = afunc(tf.add(tf.matmul(x_f, W1), b1))\n",
    "y_NN_PINNS = tf.add(tf.matmul(a1_PINNS, W2), b2)\n",
    "# note that dy_dx_NN, y_NN_PINNS and y_NN share the same weights and biases\n",
    "dy_dx_NN = tf.gradients(y_NN_PINNS, x_f)[0]\n",
    "\n",
    "f =  dy_dx_NN - y_NN_PINNS\n",
    "\n",
    "# Define loss function including the differential equation term\n",
    "MSE_u = tf.reduce_mean(tf.square(y - y_NN))\n",
    "MSE_f = tf.reduce_mean(tf.square(f))\n",
    "loss_PINNS = MSE_u + MSE_f\n",
    "optimiser_PINNS = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_PINNS)\n",
    "\n",
    "# We only use one training point (the first) to minimize MSEu\n",
    "x_data_train_PINNS = x_data_train[0:1,0:1] \n",
    "y_data_train_PINNS = y_data_train[0:1,0:1]\n",
    "\n",
    "# Run training of the physially informed neural net\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        _, c = sess.run([optimiser_PINNS, loss_PINNS ], \n",
    "                     feed_dict={x: x_data_train_PINNS, y: y_data_train_PINNS, x_f: x_data_train})\n",
    "        avg_cost += c\n",
    "        MSE_u_value, MSE_f_value = sess.run([MSE_u, MSE_f], \n",
    "                     feed_dict={x: x_data_train_PINNS, y: y_data_train_PINNS, x_f: x_data_train})\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost), \"MSE_u =\", \"{:.6f}\".format(MSE_u_value), \"MSE_f =\", \"{:.6f}\".format(MSE_f_value))\n",
    "\n",
    "    y_pred_PINN = sess.run(y_NN, feed_dict = {x:x_data, y:y_data})\n",
    "    loss_pred = sess.run(loss, feed_dict = {x:x_data, y:y_data, x_f:x_data})\n",
    "    plt.figure()\n",
    "    plt.plot(x_data.flatten(), y_data.flatten(), 'o')\n",
    "    plt.plot(x_data.flatten(), y_pred_PINN.flatten(),'r--')\n",
    "    plt.plot(x_data_train.flatten(), y_data_train.flatten(), 'bo')\n",
    "    plt.plot(x_data_train_PINNS.flatten(), y_data_train_PINNS.flatten(), 'go')\n",
    "    plt.legend([\"y\", \"y_NN\", \"y_train(x_f)\", \"y_train\"])\n",
    "    plt.title('Physically Informed Neural Network Solution')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    print(\"MSE_u on all data (N: {}): \".format(x_data.shape[0]), loss_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot solution from regular and PINN \n",
    "#===================================================\n",
    "plt.figure()\n",
    "plt.plot(x_data.flatten(), y_data.flatten(), 'o', markerfacecolor=\"None\")\n",
    "plt.plot(x_data.flatten(), y_pred_PINN.flatten(),'r--')\n",
    "plt.plot(x_data.flatten(), y_pred.flatten(),'k--')\n",
    "plt.legend([\"y\", \"y_NN\", \"y_NN_PINN\"]) \n",
    "plt.title('Solution comparison')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sympy cells to calculate f for some simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x_sp = sp.Symbol('x')\n",
    "y_sp = sp.cos(x_sp)\n",
    "dy_sp_dx = sp.diff(y_sp, x_sp)\n",
    "dy_sp_dx2 = sp.diff(dy_sp_dx, x_sp)\n",
    "#y_sp, dy_sp_dx, dy_sp_dx2\n",
    "y_sp + dy_sp_dx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sp = sp.Symbol('x')\n",
    "y_sp = sp.cos(x_sp)*sp.exp(x_sp)\n",
    "dy_sp_dx = sp.diff(y_sp, x_sp)\n",
    "dy_sp_dx2 = sp.diff(dy_sp_dx, x_sp)\n",
    "#y_sp, dy_sp_dx, dy_sp_dx2\n",
    "y_sp - dy_sp_dx + 0.5*dy_sp_dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN - The Two Element Windkessel model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will implement the two element Windkessel model, which we saw in chapter 1.2, in a Pysically Informed Neural Network. The model is characterized by the parameters resistance $R$ and compliance $C$ as before. The equation describing the model is the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\mathrm{d}p}{\\mathrm{dt}} = \\frac{Q_{\\mathrm{in}}}{C} - \\frac{p}{R C}, \n",
    "\\end{equation*}\n",
    "\n",
    "where $Q_{\\mathrm{in}}$ is the flow entering the compliant volume at a given point in time. If we use this model to produce data and then build a loss function implementing the equation above, taking the input flow and a few points sampled from the model produced pressure function we can train a PINN to appoximate the model. \n",
    "\n",
    "In the code below, $f$ refers to the sum:\n",
    "\n",
    "\\begin{equation*}\n",
    "f = \\frac{\\mathrm{d}p}{\\mathrm{dt}} - \\frac{Q_{\\mathrm{in}}}{C} + \\frac{p}{R C}\n",
    "\\end{equation*}\n",
    "\n",
    "which ideally should be equal to zero, and must be minimized to obey the physics of the problem. $u$ refers to our variable of interest, which here is $p$ used interchangeably with $P_{ao}$. \n",
    "\n",
    "Problem 2:\n",
    "- Tune the hyperparameters; number of nodes, layers, epochs, training rate, but also the number of pressure training points, number of flow training points, and try to get a good fit. Can you implement an ordinary neural network which can learn the pressure? Which can you get to perform best?\n",
    "\n",
    "For this problem a gaussian inflow is prescribed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and functions to calculate solution and flow for given model parameters\n",
    "\n",
    "Note: Rerun the entire code from this point on if you run any code in the section on 1D blood flow and then go back here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.integrate as scint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#============================================================#\n",
    "# Set up a 2 element Windkessel model for production of data #\n",
    "#============================================================#\n",
    "\n",
    "class WK2():\n",
    "    def __init__(self, pars):\n",
    "        self.R_sys = pars[\"R_sys\"]\n",
    "        self.C_sa = pars[\"C_sa\"]\n",
    "    \n",
    "    def gaussian_flow(self,t):\n",
    "        gaussian = 450.0*np.exp(-(np.mod(t,1.0)-0.2)**2 / (0.09)**2)\n",
    "        return gaussian\n",
    "    \n",
    "    def rhs(self, t, u):\n",
    "        P_ao = u[0]\n",
    "        Q_lvao = self.gaussian_flow(np.mod(t,1.0))\n",
    "        Q_aosv = P_ao/self.R_sys\n",
    "        \n",
    "        der_P_ao = (Q_lvao - Q_aosv)/self.C_sa\n",
    "        der_u = [der_P_ao]\n",
    "        \n",
    "        return der_u\n",
    "\n",
    "    def calc_all(self, t, u):\n",
    "        P_ao = u[0]\n",
    "        Q_lvao = self.gaussian_flow(np.mod(t,1.0))\n",
    "        Q_aosv =  P_ao/self.R_sys\n",
    "        \n",
    "        all_vars = locals()\n",
    "        del all_vars[\"self\"]  \n",
    "        del all_vars[\"u\"]\n",
    "\n",
    "        der_P_ao = (Q_lvao - Q_aosv)/self.C_sa\n",
    "        der_u = [der_P_ao]\n",
    "\n",
    "        return der_u, all_vars\n",
    "    \n",
    "def get_data(N, showPlots=True):\n",
    "    #Set initial condition\n",
    "    u0 = [100.0]\n",
    "    \n",
    "    #Set up a model example\n",
    "    Parameters = dict(R_sys=1.5,C_sa=1.5)\n",
    "    wkmod = WK2(Parameters)\n",
    "    Ncycles = 20\n",
    "    tmax = Ncycles*1.0 #Ncycles\n",
    "    t_eval_vals = np.linspace(0.0, tmax, N*Ncycles)\n",
    "    \n",
    "    \n",
    "    sol = scint.solve_ivp(wkmod.rhs, (0.0, tmax), u0, max_step=0.01, t_eval=t_eval_vals)\n",
    "    _, all_vars = wkmod.calc_all(sol.t, sol.y)\n",
    "    P_ao_fin_cyc = all_vars[\"P_ao\"][int((Ncycles-1)*N):]\n",
    "    t_fin = all_vars[\"t\"][0:N]\n",
    "    q_lvao = wkmod.gaussian_flow(t_fin)\n",
    "    \n",
    "    #P_np, P_q_np, integrand_q_array = calcDeltaP(t_in, integrand_q, P_in=0)\n",
    "    if showPlots:\n",
    "        plt.figure()\n",
    "        plt.plot(t_fin, P_ao_fin_cyc)\n",
    "        plt.legend([\"P_ao\"])\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"t [s]\")\n",
    "        plt.ylabel(\"P [mmHg]\")\n",
    "        plt.title(\"Pressure curve for data samples\")\n",
    "        plt.show()\n",
    "    \n",
    "    return t_fin, P_ao_fin_cyc, q_lvao, wkmod.R_sys, wkmod.C_sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and functions do define neural net, f-function and initialization of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#=============================================#\n",
    "# Functions for setting up the neural network #\n",
    "#=============================================#\n",
    "\n",
    "def net_u(t):\n",
    "    u = neural_net(t, weights, biases)\n",
    "    return u\n",
    "#===================================================\n",
    "\n",
    "def net_f_WK(t, Q, R, C):\n",
    "    \n",
    "    u = net_u(t)\n",
    "    \n",
    "    u_t = tf.gradients(u, t)\n",
    "    \n",
    "    f =  u_t + u/(R*C) - Q/C\n",
    "    \n",
    "    return f\n",
    "#===================================================\n",
    "\n",
    "def initialize_NN(layers):        \n",
    "    weights = []\n",
    "    biases = []\n",
    "    num_layers = len(layers) \n",
    "    for l in range(0, num_layers - 1):\n",
    "        W = xavier_init(size=[layers[l], layers[l+1]])\n",
    "        #W = tf.Variable(tf.random_normal([layers[l], layers[l+1]], stddev=10))\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        weights.append(W)\n",
    "        biases.append(b)        \n",
    "    return weights, biases\n",
    "#===================================================\n",
    "  \n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    out_dim = size[1]        \n",
    "    xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "    return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "#===================================================\n",
    "\n",
    "def neural_net(X, weights, biases):\n",
    "    num_layers = len(weights) + 1\n",
    "    \n",
    "    H = 2.0*(X - lb)/(ub - lb) - 1.0\n",
    "    for l in range(0, num_layers - 2):\n",
    "        W = weights[l]\n",
    "        b = biases[l]\n",
    "        H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
    "    W = weights[-1]\n",
    "    b = biases[-1]\n",
    "    Y = tf.add(tf.matmul(H, W), b)\n",
    "    return Y\n",
    "#===================================================\n",
    "\n",
    "def callback(loss):\n",
    "    print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program, setting training parameters and NN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# load data\n",
    "#===================================================\n",
    "N = 1001 # Total number of t, P values to generate\n",
    "N_train = 1 # number of training points for NN\n",
    "N_train_f = 50 # number of training points for f, i.e., input flow term and pressure term\n",
    "\n",
    "#maxP = np.max(P_data)\n",
    "t, P_data, Q_data, R, C = get_data(N, showPlots=False)\n",
    "#P_data = P_data/maxP\n",
    "#===================================================\n",
    "# set parameters for neural network and training\n",
    "#===================================================\n",
    "layers = [1, 7, 1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#===================================================\n",
    "# turn 1D array into 2D array of shape (N, 1)\n",
    "#===================================================\n",
    "t = t[:, np.newaxis]\n",
    "P_data = P_data[:, np.newaxis]\n",
    "Q_data = Q_data[:, np.newaxis]\n",
    "#===================================================\n",
    "# sample random points for training\n",
    "#===================================================\n",
    "idx = np.random.choice(t.shape[0], N_train, replace=False)\n",
    "idx_f = np.random.choice(t.shape[0], N_train_f, replace=False)\n",
    "t_train = t[idx]\n",
    "P_train = P_data[idx]\n",
    "#t_train = t[0:1,0:1]\n",
    "#P_train = P[0:1,0:1]\n",
    "t_f_train = t[idx_f]\n",
    "q_train = Q_data[idx_f]\n",
    "P_f_train = P_data[idx_f]\n",
    "\n",
    "#===================================================\n",
    "# Set up placeholder for inputs and outputs\n",
    "#===================================================\n",
    "C_tf = tf.constant(C)\n",
    "R_tf = tf.constant(R)\n",
    "\n",
    "t_tf = tf.placeholder(tf.float32, shape=[None, t.shape[1]], name='t')\n",
    "P_tf = tf.placeholder(tf.float32, shape=[None, P_data.shape[1]], name='P')\n",
    "\n",
    "t_f_tf = tf.placeholder(tf.float32, shape=[None, t.shape[1]], name='t')\n",
    "q_tf = tf.placeholder(tf.float32, shape=[None, Q_data.shape[1]], name='Q')\n",
    "\n",
    "#===================================================\n",
    "# initialize neural net and f functions\n",
    "#===================================================\n",
    "lb = t.min(0)\n",
    "ub = t.max(0)\n",
    "weights, biases = initialize_NN(layers)\n",
    "\n",
    "P_pred = net_u(t_tf)\n",
    "f_pred = net_f_WK(t_f_tf, q_tf, C_tf, R_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    P_pred_init = sess.run(P_pred, feed_dict = {t_tf:t})\n",
    "    plt.figure()\n",
    "    plt.plot(t.flatten(), P_data.flatten(), label = \"Data\")\n",
    "    plt.plot(t_f_train.flatten(), P_f_train.flatten(), 'o', label=\"Diff. eq. training points\")\n",
    "    plt.plot(t_train.flatten(), P_train.flatten(), 'o', label=\"NN Training point\")\n",
    "    plt.plot(t.flatten(), P_pred_init.flatten(), label = \"Initialization values\")\n",
    "    plt.title('Initialization values')\n",
    "    plt.xlabel('t [s]')\n",
    "    plt.ylabel('P [mmHg]')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===========================================================#\n",
    "# train the model using PINNS and GradientDescentOptimizer  #\n",
    "#===========================================================#\n",
    "batch_size = N_train\n",
    "loss = tf.reduce_mean(tf.square(P_tf - P_pred)) + tf.reduce_mean(tf.square(f_pred))\n",
    "learning_rate = 0.00012\n",
    "#learning_rate_late = 0.5\n",
    "epochs = 1000\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        _, c = sess.run([optimiser, loss], \n",
    "                     feed_dict={t_tf: t_train, P_tf: P_train, \n",
    "                                t_f_tf: t_f_train, q_tf:q_train})\n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost))\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {t_tf:t})\n",
    "    plt.figure()\n",
    "    plt.plot(t.flatten(), P_data.flatten(), label='Data')\n",
    "    plt.plot(t_train.flatten(), P_train.flatten(), 'o', label='NN Training Point')\n",
    "    plt.plot(t.flatten(), P_result.flatten(), 'k', label='PINN Solution')\n",
    "    plt.title('Physically Informed Neural Network Solution')\n",
    "    plt.xlabel('t [s]')\n",
    "    plt.ylabel('P [mmHg]')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell an alternative, faster optimization for training the network which uses the SciPy compatible functionalities of Tensorflow is set up.\n",
    "This method reinitializes for each run so it can be rerun immediately to yield different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#===========================================================#\n",
    "# train the model using PINNS and ScipyOptimizerInterface   #\n",
    "#===========================================================#\n",
    "\n",
    "optimizer = tf.contrib.opt.ScipyOptimizerInterface(loss, \n",
    "                                                   method = 'L-BFGS-B', \n",
    "                                                   options = {'maxiter': 500,\n",
    "                                                              'maxfun': 50000,\n",
    "                                                              'maxcor': 50,\n",
    "                                                              'maxls': 50,\n",
    "                                                              'ftol' : 1.0 * np.finfo(float).eps})\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf_dict = {t_tf: t_train, P_tf: P_train, \n",
    "                             t_f_tf: t_f_train, q_tf:q_train}\n",
    "    optimizer.minimize(sess, \n",
    "                       feed_dict = tf_dict,         \n",
    "                       fetches = [loss], \n",
    "                       loss_callback = callback)\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {t_tf: t})\n",
    "    plt.figure()\n",
    "    plt.plot(t.flatten(), P_data.flatten(),'b', label='Data')\n",
    "    plt.plot(t_train.flatten(), P_train.flatten(), 'yo', label='NN Training Points')\n",
    "    plt.plot(t.flatten(), P_result.flatten(), 'r', label='PINN Solution')\n",
    "    plt.legend()\n",
    "    plt.title('Physically Informed Neural Network Solution')\n",
    "    plt.xlabel('P [mmHg]')\n",
    "    plt.ylabel('t [s]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# PINNS applied on the steady state 1D equation for blood flow\n",
    "\n",
    "In this notebook we will try to solve the steady state 1D momentum equation for blood flow in rigid domains. \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right) = -\\frac{A}{\\rho}\\frac{\\partial P}{\\partial x} - \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\rho \\, A}\\,,\n",
    "\\end{equation}\n",
    "which may be reformulated to\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial P}{\\partial x}  = -\\frac{\\rho}{A} \\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right)- \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\, A^2}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "In this case we'll treat Q as given in which and we may express P(x) as:\n",
    "\n",
    "\\begin{equation}\n",
    "P\\left(x\\right)  = \\int^x -\\frac{\\rho}{A} \\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right) dx  + \\int^x- \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\, A^2} dx\\,,\n",
    "\\end{equation}\n",
    "\n",
    "Our f function is defined as \n",
    "\\begin{equation}\n",
    "f  = \\frac{\\partial P}{\\partial x} - I_c - I_f \\,,\n",
    "\\end{equation}\n",
    "where $I_c=-\\frac{\\rho}{A} \\frac{\\partial}{\\partial x } \\left(\\frac{Q^2}{A}\\right)$ and $I_f=- \\frac{8 \\, \\mu \\, \\pi \\, Q}{\\, A^2}$, in which $I_c$ and $I_f$ are source terms.\n",
    "\n",
    "## Code and functions to calculate solution and source terms for different geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3:\n",
    "- Solve the 1D problem for different geometries; \"linearTapering\", \"sine\", \"constant\" using PINNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from scipy.integrate import quad\n",
    "\n",
    "%matplotlib inline\n",
    "def sinusGeom_np(x, R0, Rmin, l):\n",
    "    r = R0 + ((R0 - Rmin)/2)*(np.cos(2*np.pi*x/l) - 1)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def sinusGeom_sp(x, R0, Rmin, l):\n",
    "    r = R0 + ((R0 - Rmin)/2)*(sp.cos(2*sp.pi*x/l) - 1)\n",
    "    \n",
    "    return r\n",
    "\n",
    "def linearTapering(x, R0, Rmin, l):\n",
    "    \n",
    "    r = R0 + (Rmin - R0)*x\n",
    "    \n",
    "    return r\n",
    "\n",
    "def integrand_friction_sp(r, mu, Q):\n",
    "    \n",
    "    A = sp.pi*r**2\n",
    "    \n",
    "    I_f_sp = -8*mu*sp.pi*Q/(A**2)\n",
    "    \n",
    "    return I_f_sp\n",
    "\n",
    "def integrand_convective_sp(r, rho, Q):\n",
    "    \n",
    "    A = sp.pi*r**2\n",
    "    \n",
    "    I_c_sp = -rho*sp.diff(Q**2/A)/A\n",
    "    \n",
    "    return I_c_sp\n",
    "\n",
    "def calcDeltaP(x, I_f, I_c, diffusive=True, convective=True, P_in=0):\n",
    "    \n",
    "    P = np.zeros(len(x))\n",
    "    P[0] = P_in\n",
    "    P_f = np.zeros(len(x))\n",
    "    P_f[0] = P_in\n",
    "    P_c = np.zeros(len(x))\n",
    "    P_c[0] = P_in\n",
    "    I_f_array = np.zeros(len(x))\n",
    "    I_c_array = np.zeros(len(x))\n",
    "    if diffusive:\n",
    "        I_f_array[0] = I_f(x[0])\n",
    "    if convective:\n",
    "        I_c_array[0] = I_c(x[0])\n",
    "    for n in range(len(x) - 1):        \n",
    "        \n",
    "        dp = 0\n",
    "        dp_f = quad(I_f, x[n], x[n + 1])[0]\n",
    "        dp_c = quad(I_c, x[n], x[n + 1])[0]\n",
    "        if diffusive:\n",
    "            dp += dp_f\n",
    "            P_f[n + 1] = P_f[n] + dp_f\n",
    "            I_f_array[n + 1] = I_f(x[n + 1])\n",
    "        if convective:\n",
    "            dp += dp_c\n",
    "            P_c[n + 1] = P_c[n] + dp_c\n",
    "            I_c_array[n + 1] = I_c(x[n + 1])\n",
    "        P[n + 1] = P[n] + dp\n",
    "    \n",
    "    return P, P_f, P_c, I_f_array, I_c_array\n",
    "\n",
    "def get1D_data(N, geometryType=\"sine\", showPlots=True, diffusive=True, convective=True):\n",
    "\n",
    "    R0 = 0.2    # [cm]\n",
    "    Rmin = 0.05 # [cm]\n",
    "    l = 1       # [cm]\n",
    "    Q = 2       # [ml/s]\n",
    "    rho = 1.05  # [g/cm^3]\n",
    "    mu = 0.035  # [P] (g/(cm s))\n",
    "    #N = 1001\n",
    "    x_np = np.linspace(0, l, N)\n",
    "    \n",
    "    if geometryType == \"sine\":\n",
    "        geomFunc_np = sinusGeom_np\n",
    "        geomFunc_sp = sinusGeom_sp\n",
    "    elif geometryType == \"linearTapering\":\n",
    "        geomFunc_np = linearTapering\n",
    "        geomFunc_sp = linearTapering\n",
    "    elif geometryType == \"constant\":\n",
    "        geomFunc_np = linearTapering\n",
    "        geomFunc_sp = linearTapering\n",
    "        R0 = Rmin\n",
    "    r_np = geomFunc_np(x_np, R0, Rmin, l)\n",
    "\n",
    "    x = sp.Symbol('x')\n",
    "    r_sp = geomFunc_sp(x, R0, Rmin, l)#R0 + ((R0 - Rmin)/2)*(sp.cos(2*sp.pi*x/l) - 1)\n",
    "\n",
    "    integrand_f = integrand_friction_sp(r_sp, mu, Q)\n",
    "    integrand_c = integrand_convective_sp(r_sp, rho, Q)\n",
    "\n",
    "    integrand_f = sp.lambdify([x], integrand_f)\n",
    "    integrand_c = sp.lambdify([x], integrand_c)\n",
    "\n",
    "    P_np, P_f_np, P_c_np, integrand_f_array, integrand_c_array = calcDeltaP(x_np, integrand_f, integrand_c,\n",
    "                                                                           diffusive=diffusive, convective=convective)\n",
    "\n",
    "    #showPlots = False\n",
    "    if showPlots:\n",
    "        plt.figure()\n",
    "        plt.plot(x_np, r_np)\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(\"r [cm]\")\n",
    "        plt.figure()\n",
    "        plt.plot(x_np, P_np/1333.2)\n",
    "        plt.plot(x_np, P_f_np/1333.2)\n",
    "        plt.plot(x_np, P_c_np/1333.2)\n",
    "        plt.legend([\"P\", \"P_f\", \"P_c\"])\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(\"P [mmHg]\")\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(x_np, integrand_f_array/1333.2)\n",
    "        plt.plot(x_np, integrand_c_array/1333.2)\n",
    "        plt.legend([\"I_f (dP_dx_f)\", \"I_c (dP_dx_c)\"])\n",
    "        #plt.plot(x_np, linearTapering_np(x_np, R0, Rmin, l))\n",
    "        plt.xlabel(\"x [cm]\")\n",
    "        plt.ylabel(\"dP_dx [mmHg/cm]\")\n",
    "    \n",
    "    return x_np, r_np, P_np, P_f_np, P_c_np, integrand_f_array, integrand_c_array\n",
    "\n",
    "#x_np, r_np, P_np, P_f_np, P_c_np, integrand_f_array, integrand_c_array = get1D_data(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and functions do define neural net, f-function and initialization of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# the remaining functions are initialized in the WK2 example\n",
    "#===================================================\n",
    "def net_f(x, I_f, I_c):\n",
    "    \n",
    "    u = net_u(x)\n",
    "    \n",
    "    u_x = tf.gradients(u, x)\n",
    "    \n",
    "    f =  u_x - I_f - I_c\n",
    "    \n",
    "    return f\n",
    "#==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#===================================================\n",
    "# load data\n",
    "#===================================================\n",
    "N = 1001 # total number of x, P values\n",
    "N_train = 10 # number of training points\n",
    "N_train_f = 20 # number of training points\n",
    "geometryType = \"linearTapering\" # [\"sine\", \"linearTapering\", \"constant\"]\n",
    "layers = [1, 5, 1]\n",
    "diffusive = True\n",
    "convective = True\n",
    "showPlots = True\n",
    "x, r, P, P_f, P_c, I_f, I_c = get1D_data(N, geometryType=geometryType, showPlots=showPlots,\n",
    "                                                         diffusive=diffusive, convective=convective)\n",
    "dyneTommHg = 1./1333.22368\n",
    "\n",
    "scaleInputs = True\n",
    "if scaleInputs:\n",
    "    P *= dyneTommHg\n",
    "    P_f *= dyneTommHg\n",
    "    P_c *= dyneTommHg\n",
    "    I_f *= dyneTommHg\n",
    "    I_c *= dyneTommHg\n",
    "    \n",
    "#===================================================\n",
    "# turn 1D array into 2D array of shape (N, 1)\n",
    "#===================================================\n",
    "x = x[:, np.newaxis]\n",
    "P = P[:, np.newaxis]\n",
    "I_f = I_f[:, np.newaxis]\n",
    "I_c = I_c[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# set parameters for neural network and training\n",
    "#===================================================\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(np.shape(I_f), np.shape(I_c))\n",
    "#===================================================\n",
    "# sample random points for training\n",
    "#===================================================\n",
    "idx = np.random.choice(x.shape[0], N_train, replace=False)\n",
    "idx_f = np.random.choice(x.shape[0], N_train_f, replace=False)\n",
    "x_train = x[idx]\n",
    "P_train = P[idx]\n",
    "#x_train = x[0:1, 0:1]\n",
    "#P_train = P[0:1, 0:1]\n",
    "\n",
    "x_train_f = x[idx_f]\n",
    "I_f_train_f = np.interp(x_train_f.flatten(), x.flatten(), I_f.flatten())\n",
    "I_f_train_f = I_f_train_f[:, np.newaxis]\n",
    "I_c_train_f = np.interp(x_train_f.flatten(), x.flatten(), I_c.flatten())\n",
    "I_c_train_f = I_c_train_f[:, np.newaxis]\n",
    "print(np.shape((I_c_train_f)))\n",
    "#===================================================\n",
    "# set up placeholder for inputs and outputs\n",
    "#===================================================\n",
    "x_tf = tf.placeholder(tf.float32, shape=[None, x.shape[1]], name='x')\n",
    "P_tf = tf.placeholder(tf.float32, shape=[None, P.shape[1]], name='P')\n",
    "\n",
    "x_f_tf = tf.placeholder(tf.float32, shape=[None, x.shape[1]], name='x')\n",
    "I_f_tf = tf.placeholder(tf.float32, shape=[None, I_f.shape[1]], name='integrand_f')\n",
    "I_c_tf = tf.placeholder(tf.float32, shape=[None, I_c.shape[1]], name='integrand_c')\n",
    "#===================================================\n",
    "# initialize neural net and f functions\n",
    "#===================================================\n",
    "lb = x.min(0)\n",
    "ub = x.max(0)\n",
    "weights, biases = initialize_NN(layers)\n",
    "\n",
    "P_pred = net_u(x_tf)\n",
    "f_pred = net_f(x_f_tf, I_f_tf, I_c_tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# plot y_pred before training\n",
    "#===================================================\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    P_pred_init = sess.run(P_pred, feed_dict = {x_tf:x_train})\n",
    "plt.figure()\n",
    "plt.plot(x.flatten(), P.flatten(), label='Data')\n",
    "plt.plot(x_train.flatten(), P_train.flatten(), 'o', label='NN Training Points')\n",
    "plt.plot(x_train.flatten(), P_pred_init.flatten(), label='Initialization values')\n",
    "plt.legend()\n",
    "plt.title('Initialization values')\n",
    "plt.xlabel('x [s]')\n",
    "plt.ylabel('P [mmHg]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================\n",
    "# train the model using PINNS and GradientDescentOptimizer\n",
    "#===================================================\n",
    "\n",
    "MSE_u = tf.reduce_mean(tf.square((P_tf - P_pred)))\n",
    "MSE_f = tf.reduce_mean(tf.square(f_pred))/100\n",
    "loss = MSE_u + MSE_f\n",
    "learning_rate_value = 0.01\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "learning_rate_value_late = 0.01#25\n",
    "epochs = 10000\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "print_every_N_batch = 1000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init) # initialize variables\n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        if epoch < 2000:\n",
    "            learning_rate_value_epoch = learning_rate_value\n",
    "        else:\n",
    "            learning_rate_value_epoch = learning_rate_value_late\n",
    "        _, c, MSE_u_value, MSE_f_value = sess.run([optimiser, loss, MSE_u, MSE_f], \n",
    "                     feed_dict={x_tf: x_train, P_tf: P_train, \n",
    "                                x_f_tf: x_train_f, I_f_tf: I_f_train_f,\n",
    "                                I_c_tf: I_c_train_f, learning_rate: learning_rate_value_epoch})\n",
    "        \n",
    "        \n",
    "        avg_cost += c\n",
    "        if epoch % print_every_N_batch == 0:\n",
    "            print(\"Epoch:\", (epoch + 1), \"cost =\", \"{:.6f}\".format(avg_cost), \"MSE_u =\", \"{:.6f}\".format(MSE_u_value), \"MSE_f =\", \"{:.6f}\".format(MSE_f_value))\n",
    "\n",
    "    P_result = sess.run(P_pred, feed_dict = {x_tf:x})\n",
    "    P_result_f = sess.run(P_pred, feed_dict = {x_tf: x_train_f})\n",
    "    plt.figure()\n",
    "    plt.plot(x.flatten(), P.flatten())\n",
    "    plt.plot(x.flatten(), P_result.flatten(), '--')\n",
    "    plt.plot(x_train.flatten(), P_train.flatten(), 'o')\n",
    "    plt.plot(x_train_f.flatten(), P_result_f.flatten(), 'o')\n",
    "    plt.title(\"PINN Solution\")\n",
    "    plt.xlabel(\"x [cm]\")\n",
    "    plt.ylabel(\"P [mmHg]\")\n",
    "    plt.legend([\"P(x)\", \"P_PINN\", \"P_train\", \"P_train_f\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
